{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330a2fb2-0008-48fd-9b89-59697272b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eedb629-a3d1-4df9-aece-265d72986e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "def tensorboard_cb(dirpath, model_name):\n",
    "    return tf.keras.callbacks.TensorBoard(os.path.join(dirpath, \n",
    "                                                       model_name, \n",
    "                                                       datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "def checkpoint_cb(dirpath, model_name, save_format=None, save_weights=False):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(dirpath, model_name + save_format), \n",
    "                                              save_best_only=True, \n",
    "                                              save_weights_only=save_weights,\n",
    "                                              monitor=\"val_loss\",\n",
    "                                              verbose=1)\n",
    "\n",
    "def early_stopping_cb(patience):\n",
    "    return tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                            restore_best_weights=True, \n",
    "                                            patience=patience,\n",
    "                                            verbose=1)\n",
    "\n",
    "def reduce_lr_cb(patience, factor):\n",
    "    return tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n",
    "                                                patience=patience, \n",
    "                                                factor=factor, \n",
    "                                                min_lr=1e-7,\n",
    "                                                verbose=1)\n",
    "\n",
    "def lr_scheduler_cb(lr_init, lr_div):\n",
    "    return tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_init * 10 ** (epoch/lr_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d4c19e-6c2b-4192-a69f-a44f0d6435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_trailing_window(data, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Creates trailing windows and corresponding horizons.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list or array, the input time series data.\n",
    "    - window_size: int, size of the trailing window.\n",
    "    - horizon: int, the number of steps to look ahead.\n",
    "\n",
    "    Returns:\n",
    "    - windows: list of arrays, trailing windows.\n",
    "    - horizons: list, corresponding horizons.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    horizons = []\n",
    "\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        target = data[i + window_size:i + window_size + horizon]\n",
    "\n",
    "        windows.append(window)\n",
    "        horizons.append(target)\n",
    "\n",
    "    return windows, horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccb8dd2-4c56-4eb9-ad74-d267dc093054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_centered_window(data, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Creates centered windows and corresponding horizons.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list or array, the input time series data.\n",
    "    - window_size: int, size of the centered window.\n",
    "    - horizon: int, the number of steps to look ahead.\n",
    "\n",
    "    Returns:\n",
    "    - windows: list of arrays, centered windows.\n",
    "    - horizons: list, corresponding horizons.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    horizons = []\n",
    "\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        center_index = i + window_size // 2\n",
    "        window = data[center_index - window_size // 2:center_index + window_size // 2]\n",
    "        target = data[center_index:center_index + horizon]\n",
    "\n",
    "        windows.append(window)\n",
    "        horizons.append(target)\n",
    "\n",
    "    return windows, horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be99dea4-aa2d-473f-a5bb-07e585d2ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trailing_window(data, num_targets, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Generate trailing windows for time series prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input time series data with columns representing features and targets\n",
    "    - window_size: int, size of the trailing window\n",
    "    - horizon: int, number of steps to forecast into the future\n",
    "    - num_targets: int, number of target variables\n",
    "\n",
    "    Returns:\n",
    "    - X: numpy array, input features (shape: [num_samples - window_size - horizon + 1, window_size, num_features])\n",
    "    - y: numpy array, target values (shape: [num_samples - window_size - horizon + 1, horizon, num_targets])\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = len(data)\n",
    "    num_features = data.shape[1] - num_targets  # Subtract num_targets for target columns\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(num_samples - window_size - horizon + 1):\n",
    "        X.append(data.iloc[i:i + window_size, :-num_targets].values)\n",
    "        y.append(data.iloc[i + window_size:i + window_size + horizon, -num_targets:].values.reshape(-1, num_targets))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4146df-aa9a-4d6e-b59d-f75596eb4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_centered_window(data, num_targets, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Generate centered windows for time series prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input time series data with columns representing features and targets\n",
    "    - window_size: int, size of the centered window (even number)\n",
    "    - horizon: int, number of steps to forecast into the future\n",
    "\n",
    "    Returns:\n",
    "    - X: numpy array, input features (shape: [num_samples - window_size + 1, window_size, num_features])\n",
    "    - y: numpy array, target values (shape: [num_samples - window_size + 1, horizon, num_targets])\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = len(data)\n",
    "    num_features = data.shape[1] - num_targets  # Subtract num_targets for target columns\n",
    "    num_targets = data.shape[1] - num_features\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure the window size is even\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be an even number for a centered window.\")\n",
    "\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    for i in range(half_window, num_samples - half_window - horizon + 1):\n",
    "        start_idx = i - half_window\n",
    "        end_idx = i + half_window - 1  # Adjust the end index\n",
    "\n",
    "        X.append(data.iloc[start_idx:end_idx + 1, :-num_targets].values)\n",
    "        y.append(data.iloc[i:i + horizon, -num_targets:].values.reshape(-1, num_targets))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263ff114-2fea-4b4a-990b-99f97fcf34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_2d_3d_data(data, scaler=None, norm=False, stand=False):\n",
    "    \"\"\"\n",
    "    Scale 2D or 3D data using Min-Max scaling or standardization.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array, input data (2D or 3D)\n",
    "    - scaler: object, pre-defined scaler\n",
    "    - norm: bool, whether to perform Min-Max scaling\n",
    "    - stand: bool, whether to perform standardization\n",
    "\n",
    "    Returns:\n",
    "    - If norm=True: Tuple of (scaler, scaled_data)\n",
    "    - If stand=True: Tuple of (scaler, standardized_data)\n",
    "    - If neither norm nor stand is True: None\n",
    "    \"\"\"\n",
    "\n",
    "    original_shape = data.shape if len(data.shape) == 3 else None\n",
    "\n",
    "    # Check if the input data is 3D, and reshape to 2D if necessary\n",
    "    if original_shape:\n",
    "        data = data.reshape((original_shape[0] * original_shape[1], original_shape[2]))\n",
    "\n",
    "    scaler = scaler\n",
    "    scaled_data = None\n",
    "\n",
    "    if norm:\n",
    "        if scaler is None:\n",
    "            # Min-Max scaling\n",
    "            scaler = MinMaxScaler().fit(data)\n",
    "        scaled_data = scaler.transform(data)\n",
    "\n",
    "    elif stand:\n",
    "        if scaler is None:\n",
    "            # Standardization\n",
    "            scaler = StandardScaler().fit(data)\n",
    "        scaled_data = scaler.transform(data)\n",
    "\n",
    "    # Reshape the scaled data back to 3D if the input was originally 3D\n",
    "    if original_shape and scaled_data is not None:\n",
    "        scaled_data = scaled_data.reshape(original_shape)\n",
    "\n",
    "    return scaler, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccae431d-bf26-4a11-9354-bd2bcf11664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performant_features_targets(features, targets, batch_size=32, pre=False, map_func=None, shuffle=False, shuffle_size=None):\n",
    "    \"\"\"\n",
    "    Create a performant dataset for escalating efficiency during training/evaluating a model.\n",
    "\n",
    "    Parameters:\n",
    "    - features: numpy array, input features\n",
    "    - targets: numpy array, target values\n",
    "    - batch_size: int, batch size for the dataset\n",
    "    - pre: bool, whether to apply a preprocessing function\n",
    "    - map_func: function, preprocessing function to be applied if pre=True\n",
    "    - shuffle: bool, whether to shuffle the dataset\n",
    "    - shuffle_size: int, size of the shuffle buffer\n",
    "\n",
    "    Returns:\n",
    "    - tf.data.Dataset: TensorFlow dataset\n",
    "    \"\"\"\n",
    "\n",
    "    features_tensor = tf.data.Dataset.from_tensor_slices(tf.cast(features, dtype=tf.float32))\n",
    "    targets_tensor = tf.data.Dataset.from_tensor_slices(tf.cast(targets, dtype=tf.float32))\n",
    "    dataset_tensor = tf.data.Dataset.zip((features_tensor, targets_tensor))\n",
    "\n",
    "    if pre:\n",
    "        dataset_tensor = dataset_tensor.map(map_func=map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset_tensor = dataset_tensor.shuffle(shuffle_size)\n",
    "\n",
    "    dataset_tensor = dataset_tensor.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3e73d7-894a-4ccc-94d1-e7c1763b2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performant_dataset(dataset, batch_size=32, pre=False, map_func=None, shuffle=False, shuffle_size=None):\n",
    "    \"\"\"\n",
    "    Create a performant dataset with optional preprocessing and shuffling.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: tf.data.Dataset, input dataset\n",
    "    - batch_size: int, batch size for the dataset\n",
    "    - pre: bool, whether to apply a preprocessing function\n",
    "    - map_func: function, preprocessing function to be applied if pre=True\n",
    "    - shuffle: bool, whether to shuffle the dataset\n",
    "    - shuffle_size: int, size of the shuffle buffer\n",
    "\n",
    "    Returns:\n",
    "    - tf.data.Dataset: TensorFlow dataset\n",
    "    \"\"\"\n",
    "\n",
    "    if pre:\n",
    "        dataset = dataset.map(map_func=map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1e67f-cfc4-4565-b91a-1ae5eda88c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_len = [len(sentence.split()) for sentence in train_sentences]\n",
    "mean_sentence_len = np.mean(sentence_len)\n",
    "\n",
    "max_vocab_length = 10000\n",
    "max_length = int(np.percentile(sentence_len, 95))\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)\n",
    "\n",
    "# text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "text_embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "                           output_dim=128,\n",
    "                           input_length=max_length,\n",
    "                           mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097464e-84f9-4363-bf9c-5026fd13804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,          # Rescale pixel values to [0, 1]\n",
    "    rotation_range=20,       # Random rotation up to 20 degrees\n",
    "    width_shift_range=0.2,   # Randomly shift images horizontally by up to 20% of the width\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height\n",
    "    shear_range=0.2,         # Shear intensity (shear angle in radians)\n",
    "    zoom_range=0.2,          # Random zoom up to 20%\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    fill_mode='nearest'      # Strategy for filling in newly created pixels after rotation or shifting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd50c4-0fa7-4b7f-84dd-e7e3c42b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(your_image_array)\n",
    "augmented_images_batch = datagen.flow(your_image_array, batch_size=batch_size, shuffle=False)\n",
    "augmented_images = augmented_images_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8257bf6-8dfd-4cf9-bd14-216174367913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_aug_img(dirpath, img_size, class_type, batch_size=32):\n",
    "    return datagen.flow_from_directory(\n",
    "        dirpath,\n",
    "        target_size=(img_size, img_size),   # Target size of the images\n",
    "        batch_size=batch_size,              # Batch size\n",
    "        class_mode=class_type               # Type of classification task (binary, categorical)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52db7db-8367-414d-ae1a-ec47c758fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_augmentation_layer(img_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomHeight(0.2),\n",
    "        tf.keras.layers.RandomWidth(0.2),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.Resizing(img_size, img_size)\n",
    "    ], name=\"img_augmentation_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ef1ebe-e07c-48af-9504-3fcbd74dc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_image_dataset_from_directory(filepath, label_mode, img_size, batch_size, shuffle=False):\n",
    "    img_dataset = tf.keras.preprocessing.image_dataset_from_directory(filepath,\n",
    "                                                                      label_mode=label_mode,\n",
    "                                                                      batch_size=batch_size,\n",
    "                                                                      image_size=(img_size, img_size),\n",
    "                                                                      shuffle=shuffle)\n",
    "\n",
    "    return img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c219b3de-fa35-4da3-8224-621ba6db324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ts_rg_model(actual_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Evaluate a time series/regression model for forecasting population.\n",
    "\n",
    "    Parameters:\n",
    "    - actual_values: 1D array or list of actual population values.\n",
    "    - predicted_values: 1D array or list of predicted population values.\n",
    "\n",
    "    Returns:\n",
    "    - evaluation_results: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    actual_values = np.array(actual_values)\n",
    "    predicted_values = np.array(predicted_values)\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(actual_values, predicted_values)\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(actual_values, predicted_values)\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mask = actual_values != 0  # Avoid division by zero\n",
    "    mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)[mask]) * 100\n",
    "\n",
    "    # R-squared (R2)\n",
    "    r2 = r2_score(actual_values, predicted_values)\n",
    "\n",
    "    evaluation_results = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aac89fe-c1e8-46c5-9249-d2e039bf3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model for binary/multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: 1D array or list of actual values.\n",
    "    - y_preds: 1D array or list of predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - evaluation_results: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_preds)\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(y_true, y_preds, average=\"weighted\", zero_division=1)\n",
    "    return {\"acc\": acc,\n",
    "            \"pre\": pre,\n",
    "            \"rec\": rec,\n",
    "            \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72787674-04ce-491f-bc06-5e826b3cf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = np.abs(48-len(*preds*))\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "846f83cc-1931-42e7-ab8f-866cba789b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ts_rg_preds_interval(y_true, y_preds, offset, figsize=(10, 7)):\n",
    "    \"\"\"\n",
    "    Plot a graph with a defined offset to visualize the 95% confidence of the model's predictions versus actual.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: 1D array or list of actual values.\n",
    "    - y_preds: 1D array or list of predicted values.\n",
    "    - offset: The duration to visualize.\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_preds\n",
    "    standard_errors = np.std(residuals)\n",
    "    margin_of_errors = 1.96 * standard_errors\n",
    "    upper = y_preds + margin_of_errors\n",
    "    lower = y_preds - margin_of_errors\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(y_true[offset:], label=\"Actual\", color=\"green\")\n",
    "    plt.plot(y_preds[offset:], label=\"Preds\", color=\"blue\")\n",
    "    plt.fill_between(range(len(y_true[offset:])), lower[offset:], upper[offset:], color=\"lightgrey\", label=\"Preds Interval\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Duration\")\n",
    "    plt.ylabel(\"Targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603fd1f8-205a-4d59-9b06-2252ce2b57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_timer(model, samples):\n",
    "    \"\"\"\n",
    "    Times how long a model takes to make predictions on samples.\n",
    "\n",
    "    Parameters:\n",
    "    - model: tf.keras.Model, the trained model\n",
    "    - samples: numpy array, input samples for prediction\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Total time taken, time per prediction\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    model.predict(samples)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    time_per_pred = total_time / len(samples) if len(samples) > 0 else np.nan\n",
    "    \n",
    "    return total_time, time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28130f54-af9b-43f0-8795-9f044bb7962f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
